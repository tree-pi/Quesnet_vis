#[E]Self-Supervised Exploration via Disagreement
What is the algorithm?
	_a_ RL with intrinsic reward derived from world model ensemble's prediction variance. (not using model confidence itself bc deep NNs usually overfit). 
		_specification...
		... What is the reward?
			_a_ Variance across the prediction from all models in the ensemble.
		... What is the training process?
			_specification...
			... What is the policy optimization algorithm?
				_a_ Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.
					_ How to optimize for multi-step reward horizon?
						_comment_ Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.
			... How to train the world model?
				_a_ Minimize the prediction error given the previous action via direct gradient.  
				_specify_ What is ensemble?
					_a_ A set of models with different initialization and a subset of full training data. 5 models in this paper.
				_specify_ What is the feature space?
					_a_  Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.
			end_
	_ How is the algorithm different from other similar self-supervising algorithms?
		_ What are the improvements on previous algorithm?
			_answers...
			... More stable in a stochastic world with high dimensional noises generated by agent's actions,  compared to #[P]Chuaet2018-Deep, #[P]Houthfoot2016-Variational and #[P]Pathak2017_Curiosity-driven. "If the source is ultimately unpredictable like a TV changes its channel randomly whenever the controller is touched upon, then it should induce infinite attraction to an error-driven learner."
			... More sample-efficient in a world with high-variance feedback, compared to prediction error based algorithms, compared to #[P]Oudeyer2009_what.
			end_

What is the task?
	_a_ Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.
		_specification_ What is the MNIST task?
			_a_ A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.
				_ Why design this task?
					_ The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.
		_ Why choosing these tasks? 
			_ Do they offer a good coverage of task space?
	_generalization_ Can this algorithm generalize to other tasks?
		_a_ They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).

How good is the performance?
	_ How to evaluate?
		_a_ Better external reward gained, compared with alternative models.
	_specify...
	... How good is the performance in non-stochastic environment?
		_a_ As a baseline check, as good as other internaly driven model. (sanity check) 
	... How good is the performance in stochastic environment?
		_a_ The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.

	