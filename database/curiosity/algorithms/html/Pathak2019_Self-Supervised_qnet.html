<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 100%;
            height: 100%;
            background-color: #222222;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "label": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "label": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "How to optimize for multi-step reward horizon?", "label": "How to optimize for multi-step reward horizon?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "Why choosing these tasks?", "label": "Why choosing these tasks?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "label": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "A set of models with different initialization and a subset of full training data. 5 models in this paper.", "label": "A set of models with different initialization and a subset of full training data. 5 models in this paper.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "white"}, "id": "How good is the performance?", "label": "How good is the performance?", "reference": "Ep38041", "shape": "dot", "size": 20}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What is the policy optimization algorithm?", "label": "What is the policy optimization algorithm?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "How good is the performance in stochastic environment?", "label": "How good is the performance in stochastic environment?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "How to evaluate?", "label": "How to evaluate?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "Do they offer a good coverage of task space?", "label": "Do they offer a good coverage of task space?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "Why design this task?", "label": "Why design this task?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).", "label": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "How is the algorithm different from other similar self-supervising algorithms?", "label": "How is the algorithm different from other similar self-supervising algorithms?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "white"}, "id": "What is the algorithm?", "label": "What is the algorithm?", "reference": "Ep38041", "shape": "dot", "size": 20}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "How to train the world model?", "label": "How to train the world model?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "white"}, "id": "What is the task?", "label": "What is the task?", "reference": "Ep38041", "shape": "dot", "size": 20}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "Variance across the prediction from all models in the ensemble.", "label": "Variance across the prediction from all models in the ensemble.", "reference": "Ep38041", "shape": "dot", "size": 20}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "Minimize the prediction error given the previous action via direct gradient.", "label": "Minimize the prediction error given the previous action via direct gradient.", "reference": "Ep38041", "shape": "dot", "size": 20}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What is the MNIST task?", "label": "What is the MNIST task?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "label": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "How good is the performance in non-stochastic environment?", "label": "How good is the performance in non-stochastic environment?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What are the improvements on previous algorithm?", "label": "What are the improvements on previous algorithm?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "As a baseline check, as good as other internaly driven model. (sanity check)", "label": "As a baseline check, as good as other internaly driven model. (sanity check)", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What is ensemble?", "label": "What is ensemble?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What is the reward?", "label": "What is the reward?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "Better external reward gained, compared with alternative models.", "label": "Better external reward gained, compared with alternative models.", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "What is the feature space?", "label": "What is the feature space?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "white"}, "id": "Can this algorithm generalize to other tasks?", "label": "Can this algorithm generalize to other tasks?", "reference": "Ep38041", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "white"}, "id": "What is the training process?", "label": "What is the training process?", "reference": "Ep38041", "shape": "dot", "size": 15}]);
        edges = new vis.DataSet([{"from": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "", "to": "Why design this task?"}, {"from": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "a", "to": "What is the MNIST task?"}, {"from": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "", "to": "How to optimize for multi-step reward horizon?"}, {"from": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "a", "to": "What is the policy optimization algorithm?"}, {"from": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "label": "a", "to": "What is the feature space?"}, {"from": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "label": "a", "to": "How good is the performance in stochastic environment?"}, {"from": "How to optimize for multi-step reward horizon?", "label": "comment", "to": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those."}, {"from": "Why choosing these tasks?", "label": "", "to": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm."}, {"from": "Why choosing these tasks?", "label": "", "to": "Do they offer a good coverage of task space?"}, {"from": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "label": "", "to": "Why design this task?"}, {"from": "A set of models with different initialization and a subset of full training data. 5 models in this paper.", "label": "a", "to": "What is ensemble?"}, {"from": "How good is the performance?", "label": "a", "to": "How good is the performance in stochastic environment?"}, {"from": "How good is the performance?", "label": "", "to": "How to evaluate?"}, {"from": "How good is the performance?", "label": "specify", "to": "How good is the performance in non-stochastic environment?"}, {"from": "What is the policy optimization algorithm?", "label": "specification", "to": "What is the training process?"}, {"from": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "a", "to": "What is the task?"}, {"from": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "specification", "to": "What is the MNIST task?"}, {"from": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "a", "to": "What is the training process?"}, {"from": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "a", "to": "What is the algorithm?"}, {"from": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "specification", "to": "What is the reward?"}, {"from": "How to evaluate?", "label": "a", "to": "Better external reward gained, compared with alternative models."}, {"from": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).", "label": "a", "to": "Can this algorithm generalize to other tasks?"}, {"from": "How is the algorithm different from other similar self-supervising algorithms?", "label": "", "to": "What is the algorithm?"}, {"from": "How is the algorithm different from other similar self-supervising algorithms?", "label": "", "to": "What are the improvements on previous algorithm?"}, {"from": "How to train the world model?", "label": "specify", "to": "What is the feature space?"}, {"from": "How to train the world model?", "label": "a", "to": "Minimize the prediction error given the previous action via direct gradient."}, {"from": "How to train the world model?", "label": "specify", "to": "What is ensemble?"}, {"from": "How to train the world model?", "label": "comment", "to": "What is the training process?"}, {"from": "What is the task?", "label": "generalization", "to": "Can this algorithm generalize to other tasks?"}, {"from": "Variance across the prediction from all models in the ensemble.", "label": "a", "to": "What is the reward?"}, {"from": "How good is the performance in non-stochastic environment?", "label": "a", "to": "As a baseline check, as good as other internaly driven model. (sanity check)"}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": true
    },
    "physics": {
        "barnesHut": {
            "avoidOverlap": 0,
            "centralGravity": 0.3,
            "damping": 0.09,
            "gravitationalConstant": -8000,
            "springConstant": 0.001,
            "springLength": 100
        },
        "enabled": true,
        "stabilization": {
            "enabled": false,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        // default to using dot shape for nodes
        options.nodes = {
            shape: "dot"
        }
        

        network = new vis.Network(container, data, options);

        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>