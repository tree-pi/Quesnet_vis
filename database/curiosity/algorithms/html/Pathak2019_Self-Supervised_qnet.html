<html>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.css" type="text/css" />
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis-network.min.js"> </script>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->

<style type="text/css">

        #mynetwork {
            width: 100%;
            height: 100%;
            background-color: #fafafa;
            border: 1px solid lightgray;
            position: relative;
            float: left;
        }

        

        

        
</style>

</head>

<body>
<div id = "mynetwork"></div>


<script type="text/javascript">

    // initialize global variables.
    var edges;
    var nodes;
    var network; 
    var container;
    var options, data;

    
    // This method is responsible for drawing the graph, returns the drawn network
    function drawGraph() {
        var container = document.getElementById('mynetwork');
        
        

        // parsing and collecting nodes and edges from the python
        nodes = new vis.DataSet([{"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "reference": "Ep4056", "shape": "dot", "size": 12}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "label": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How to train the world model?", "label": "How to train the world model?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How good is the performance in non-stochastic environment?", "label": "How good is the performance in non-stochastic environment?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is the training process?", "label": "What is the training process?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "black"}, "id": "What is the task?", "label": "What is the task?", "reference": "Ep4056", "shape": "dot", "size": 20}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What are the improvements on previous algorithm?", "label": "What are the improvements on previous algorithm?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "\"If the source is ultimately unpredictable like a TV changes its channel randomly whenever the controller is touched upon, then it should induce infinite attraction to an error-driven learner.\"", "feature": "Question", "font": {"color": "black"}, "id": "More stable in a stochastic world with high dimensional noises generated by agent\u0027s actions,  compared to #[P]Chuaet2018-Deep, #[P]Houthfoot2016-Variational and #[P]Pathak2017_Curiosity-driven. ", "label": "More stable in a stochastic world with high dimensional noises generated by agent\u0027s actions,  compared to #[P]Chuaet2018-Deep, #[P]Houthfoot2016-Variational and #[P]Pathak2017_Curiosity-driven. ", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "Variance across the prediction from all models in the ensemble.", "label": "Variance across the prediction from all models in the ensemble.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How is the algorithm different from other similar self-supervising algorithms?", "label": "How is the algorithm different from other similar self-supervising algorithms?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is the MNIST task?", "label": "What is the MNIST task?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "Why choosing these tasks?", "label": "Why choosing these tasks?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "Can this algorithm generalize to other tasks?", "label": "Can this algorithm generalize to other tasks?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How to evaluate?", "label": "How to evaluate?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "label": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How good is the performance in stochastic environment?", "label": "How good is the performance in stochastic environment?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "label": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "label": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is the feature space?", "label": "What is the feature space?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "Why design this task?", "label": "Why design this task?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "More sample-efficient in a world with high-variance feedback, compared to prediction error based algorithms, compared to #[P]Oudeyer2009_what.", "label": "More sample-efficient in a world with high-variance feedback, compared to prediction error based algorithms, compared to #[P]Oudeyer2009_what.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "black"}, "id": "What is the algorithm?", "label": "What is the algorithm?", "reference": "Ep4056", "shape": "dot", "size": 20}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is ensemble?", "label": "What is ensemble?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is the reward?", "label": "What is the reward?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "A set of models with different initialization and a subset of full training data. 5 models in this paper.", "label": "A set of models with different initialization and a subset of full training data. 5 models in this paper.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "Better external reward gained, compared with alternative models.", "label": "Better external reward gained, compared with alternative models.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "Minimize the prediction error given the previous action via direct gradient.", "label": "Minimize the prediction error given the previous action via direct gradient.", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "Do they offer a good coverage of task space?", "label": "Do they offer a good coverage of task space?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).", "label": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote).", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "How to optimize for multi-step reward horizon?", "label": "How to optimize for multi-step reward horizon?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "orange", "content": "", "feature": "Question", "font": {"color": "black"}, "id": "What is the policy optimization algorithm?", "label": "What is the policy optimization algorithm?", "reference": "Ep4056", "shape": "dot", "size": 15}, {"color": "red", "content": "", "feature": "Root question", "font": {"color": "black"}, "id": "How good is the performance?", "label": "How good is the performance?", "reference": "Ep4056", "shape": "dot", "size": 20}, {"color": "blue", "content": "", "feature": "Answer", "font": {"color": "black"}, "id": "As a baseline check, as good as other internaly driven model. (sanity check)", "label": "As a baseline check, as good as other internaly driven model. (sanity check)", "reference": "Ep4056", "shape": "dot", "size": 20}]);
        edges = new vis.DataSet([{"from": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "answer", "to": "What is the MNIST task?"}, {"from": "A toy task where the input MNIST digit will predict the transition to next digit. There are two extreme cases: one only deterministic and one only pure random.", "label": "", "to": "Why design this task?"}, {"from": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "label": "", "to": "Why design this task?"}, {"from": "How to train the world model?", "label": "answer", "to": "Minimize the prediction error given the previous action via direct gradient."}, {"from": "How to train the world model?", "label": "specification", "to": "What is the feature space?"}, {"from": "How to train the world model?", "label": "specification", "to": "What is the training process?"}, {"from": "How to train the world model?", "label": "specification", "to": "What is ensemble?"}, {"from": "How good is the performance in non-stochastic environment?", "label": "specification", "to": "How good is the performance?"}, {"from": "How good is the performance in non-stochastic environment?", "label": "answer", "to": "As a baseline check, as good as other internaly driven model. (sanity check)"}, {"from": "What is the training process?", "label": "specification", "to": "What is the policy optimization algorithm?"}, {"from": "What is the training process?", "label": "specification", "to": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit)."}, {"from": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "specification", "to": "What is the reward?"}, {"from": "RL with intrinsic reward derived from world model ensemble\u0027s prediction variance. (not using model confidence itself bc deep NNs usually overfit).", "label": "answer", "to": "What is the algorithm?"}, {"from": "What is the task?", "label": "generalization", "to": "Can this algorithm generalize to other tasks?"}, {"from": "What is the task?", "label": "answer", "to": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm."}, {"from": "What are the improvements on previous algorithm?", "label": "", "to": "How is the algorithm different from other similar self-supervising algorithms?"}, {"from": "What are the improvements on previous algorithm?", "label": "answers", "to": "More stable in a stochastic world with high dimensional noises generated by agent\u0027s actions,  compared to #[P]Chuaet2018-Deep, #[P]Houthfoot2016-Variational and #[P]Pathak2017_Curiosity-driven. "}, {"from": "What are the improvements on previous algorithm?", "label": "answers", "to": "More sample-efficient in a world with high-variance feedback, compared to prediction error based algorithms, compared to #[P]Oudeyer2009_what."}, {"from": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "specification", "to": "What is the MNIST task?"}, {"from": "Multiple tasks including Atari games, 3D navigation in Unity, MNIST, object manipulation in Mujoco and real world robotic manipulation task using Sawyer arm.", "label": "", "to": "Why choosing these tasks?"}, {"from": "Variance across the prediction from all models in the ensemble.", "label": "answer", "to": "What is the reward?"}, {"from": "How is the algorithm different from other similar self-supervising algorithms?", "label": "", "to": "What is the algorithm?"}, {"from": "Why choosing these tasks?", "label": "", "to": "Do they offer a good coverage of task space?"}, {"from": "Can this algorithm generalize to other tasks?", "label": "answer", "to": "They cover a good range of concerns for self-driven exploration models, including: 1) should assign low intrinsic reward to unpredictable states same as overly predictable state; 2) should not be overly attracted to a self-adminstered stochasticity (TV with remote)."}, {"from": "How to evaluate?", "label": "", "to": "How good is the performance?"}, {"from": "How to evaluate?", "label": "answer", "to": "Better external reward gained, compared with alternative models."}, {"from": "The disagreement with policy gradient always learns better and faster than other intrinsic reward driven algorithms.", "label": "answer", "to": "How good is the performance in stochastic environment?"}, {"from": "How good is the performance in stochastic environment?", "label": "specification", "to": "How good is the performance?"}, {"from": " Random feature space in all video games and navigation, classification features in MNIST and ImageNet-pretrained ResNet-18 features in real world robot experiments.", "label": "answer", "to": "What is the feature space?"}, {"from": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "label": "comment", "to": "How to optimize for multi-step reward horizon?"}, {"from": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "", "to": "How to optimize for multi-step reward horizon?"}, {"from": "Using direct gradients to maximize internal reward given the current world model (not TD!!). Using epsilon greedy in continuous action space as in #[P](Lillicrap et al., 2016) and straight-through estimator #[P](Bengio et al., 2013) in discrete action space.", "label": "answer", "to": "What is the policy optimization algorithm?"}, {"from": "What is ensemble?", "label": "answer", "to": "A set of models with different initialization and a subset of full training data. 5 models in this paper."}]);

        // adding nodes and edges to the graph
        data = {nodes: nodes, edges: edges};

        var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": false,
            "type": "continuous"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": false,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};
        
        

        // default to using dot shape for nodes
        options.nodes = {
            shape: "dot"
        }
        

        network = new vis.Network(container, data, options);

        


        

        return network;

    }

    drawGraph();

</script>
</body>
</html>