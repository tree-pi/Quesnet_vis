{"nodes": [{"id": "Q33487", "label": "Why\tneed intrinsic motivation?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q81067", "label": "What is the example -- robot?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q83765", "label": "Infants seem to do goal-less exploration to learn skills useful for the future #[P]Ryan2000_Intrinsic, #[P]Smith2005_The.", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q2658", "label": "Can intrinsic reward algorithms behave better or similarly good in those environments?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q36101", "label": "What is the algorithm?", "paper": ["Burda2019_Large", "Pathak2019_Self-Supervised"], "color": "green", "size": 20}, {"id": "Q81184", "label": "What is the world model (forward dynamics predictor)?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q12041", "label": "How to embed the state?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q17760", "label": "What is the discounting horizon?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q16154", "label": "What are the tasks?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q11336", "label": "How to define the end of the game, if it is not treated as an external reward signal?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q22243", "label": "How good is the performance?", "paper": ["Burda2019_Large", "Houthooft2016_VIME", "Pathak2019_Self-Supervised"], "color": "tomato", "size": 30}, {"id": "Q21816", "label": "How to evaluate?", "paper": ["Burda2019_Large", "Pathak2019_Self-Supervised"], "color": "green", "size": 20}, {"id": "Q98466", "label": "What kind of tasks will the purely internal driven agent also learn to gain more external rewards?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q30729", "label": "What is the effect of the different feature learning variants (embeddings) on these behaviors?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q46231", "label": "Why do random features perform so well?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q12070", "label": "Can IDF get more stable and perform better?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q53895", "label": "In what situation will the algorithm conceptually doom to fail?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q20919", "label": "What is the generalization task?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q10726", "label": "Is the transferred model better than a model training from scratch in the new environment?", "paper": ["Burda2019_Large"], "color": "blue", "size": 10}, {"id": "Q59879", "label": "What type of task is this algorithm specialized at?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q94511", "label": "What are the other algorithms used in continuous state-action tasks previously?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q12866", "label": "What is the problem of the previous algorithms?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q26986", "label": "What is this algorithm #[E]VIME?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q55473", "label": "What is the state model (world model)?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q73954", "label": "What is Bayesian neural network?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q55676", "label": "What is the reward?", "paper": ["Houthooft2016_VIME", "Pathak2019_Self-Supervised"], "color": "green", "size": 20}, {"id": "Q58660", "label": "How to train the world model?", "paper": ["Houthooft2016_VIME", "Pathak2019_Self-Supervised"], "color": "green", "size": 20}, {"id": "Q78756", "label": "How to train the policy?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q64865", "label": "What are the other algorithms also using curioisty-driven rewards?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q2710", "label": "How does #[E]VIME relate with other curiosity-driven algorithms?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q86402", "label": "What is the task?", "paper": ["Houthooft2016_VIME", "Pathak2019_Self-Supervised"], "color": "green", "size": 20}, {"id": "Q18469", "label": "What are features of these tasks?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q3371", "label": "What to compare with?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q28642", "label": "What is the performance difference from other algorithms?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q87817", "label": "What are the effects of the parameters?", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q72257", "label": "Why in the purely curiosity driven works in like #[ATR]Pathak the external reward is not sacrificed...or is it? They didn't compare!", "paper": ["Houthooft2016_VIME"], "color": "blue", "size": 10}, {"id": "Q903", "label": "What is the training process?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q30384", "label": "What is the policy optimization algorithm?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q62788", "label": "How to optimize for multi-step reward horizon?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q6457", "label": "Could apply the policy optimizer recursively at every future step of simulation. Furthermore could make the world model good at predicting multiple future steps with LSTM. This paper not addressing those.", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q74786", "label": "What is ensemble?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q22262", "label": "What is the feature space?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q22730", "label": "How is the algorithm different from other similar self-supervising algorithms?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q27893", "label": "What are the improvements on previous algorithm?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q38350", "label": "More stable in a stochastic world with high dimensional noises generated by agent's actions,  compared to #[P]Chuaet2018-Deep, #[P]Houthfoot2016-Variational and #[P]Pathak2017_Curiosity-driven. ", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q26879", "label": "More sample-efficient in a world with high-variance feedback, compared to prediction error based algorithms, compared to #[P]Oudeyer2009_what.", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q3591", "label": "What is the MNIST task?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q2327", "label": "Why design this task?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q45821", "label": "The MNIST images provides noisy environment; the two contrasting states should indicate similarly low learning motivation since they are either not very predictable.", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q62379", "label": "Why choosing these tasks?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q28476", "label": "Do they offer a good coverage of task space?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q92467", "label": "Can this algorithm generalize to other tasks?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q29921", "label": "How good is the performance in non-stochastic environment?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}, {"id": "Q97365", "label": "How good is the performance in stochastic environment?", "paper": ["Pathak2019_Self-Supervised"], "color": "blue", "size": 10}], "edges": [{"from": "Q33487", "to": "Q81067", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q81067", "to": "Q83765", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q81067", "to": "Q2658", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q36101", "to": "Q81184", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q81184", "to": "Q12041", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q36101", "to": "Q17760", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q16154", "to": "Q11336", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22243", "to": "Q21816", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q21816", "to": "Q98466", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q21816", "to": "Q30729", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q30729", "to": "Q46231", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q46231", "to": "Q12070", "label": "follow-up", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q21816", "to": "Q53895", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q21816", "to": "Q20919", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q21816", "to": "Q10726", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q59879", "to": "Q94511", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q94511", "to": "Q12866", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q55473", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q55473", "to": "Q73954", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q55676", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q58660", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q78756", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q64865", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q26986", "to": "Q2710", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q86402", "to": "Q18469", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22243", "to": "Q3371", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q3371", "to": "Q28642", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22243", "to": "Q87817", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q87817", "to": "Q72257", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q36101", "to": "Q55676", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q36101", "to": "Q903", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q903", "to": "Q30384", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q30384", "to": "Q62788", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q62788", "to": "Q6457", "label": "comment", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q903", "to": "Q58660", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q58660", "to": "Q74786", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q58660", "to": "Q22262", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q36101", "to": "Q22730", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22730", "to": "Q27893", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q27893", "to": "Q38350", "label": "answers", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q27893", "to": "Q26879", "label": "answers", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q86402", "to": "Q3591", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q3591", "to": "Q2327", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q2327", "to": "Q45821", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q86402", "to": "Q62379", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q62379", "to": "Q28476", "label": "", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q86402", "to": "Q92467", "label": "generalization", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22243", "to": "Q29921", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}, {"from": "Q22243", "to": "Q97365", "label": "specification", "font": {"align": "middle"}, "arrows": "to, middle"}]}